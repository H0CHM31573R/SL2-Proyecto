{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee23f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724e3b2",
   "metadata": {},
   "source": [
    "Datos obtenidos de: https://www.kaggle.com/code/tmishinev/nlp-depression-tweets-keras-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2bc2c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv(\"Mental-Health-Twitter.csv\")\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae62447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>favourites</th>\n",
       "      <th>statuses</th>\n",
       "      <th>retweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>637894677824413696</td>\n",
       "      <td>Sun Aug 30 07:48:37 +0000 2015</td>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>637890384576778240</td>\n",
       "      <td>Sun Aug 30 07:31:33 +0000 2015</td>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>637749345908051968</td>\n",
       "      <td>Sat Aug 29 22:11:07 +0000 2015</td>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>637696421077123073</td>\n",
       "      <td>Sat Aug 29 18:40:49 +0000 2015</td>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>637696327485366272</td>\n",
       "      <td>Sat Aug 29 18:40:26 +0000 2015</td>\n",
       "      <td>It’s hard to say whether packing lists are mak...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>637692793083817985</td>\n",
       "      <td>Sat Aug 29 18:26:24 +0000 2015</td>\n",
       "      <td>Making packing lists is my new hobby... #movin...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>637691649943072772</td>\n",
       "      <td>Sat Aug 29 18:21:51 +0000 2015</td>\n",
       "      <td>At what point does keeping stuff for nostalgic...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>637689418472652800</td>\n",
       "      <td>Sat Aug 29 18:12:59 +0000 2015</td>\n",
       "      <td>Currently in the finding-boxes-of-random-shit ...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>637687177946734592</td>\n",
       "      <td>Sat Aug 29 18:04:05 +0000 2015</td>\n",
       "      <td>Can't be bothered to cook, take away on the wa...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>637684866906255360</td>\n",
       "      <td>Sat Aug 29 17:54:54 +0000 2015</td>\n",
       "      <td>RT @itventsnews: ITV releases promo video for ...</td>\n",
       "      <td>1013187241</td>\n",
       "      <td>84</td>\n",
       "      <td>211</td>\n",
       "      <td>251</td>\n",
       "      <td>837</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             post_id                    post_created  \\\n",
       "0           0  637894677824413696  Sun Aug 30 07:48:37 +0000 2015   \n",
       "1           1  637890384576778240  Sun Aug 30 07:31:33 +0000 2015   \n",
       "2           2  637749345908051968  Sat Aug 29 22:11:07 +0000 2015   \n",
       "3           3  637696421077123073  Sat Aug 29 18:40:49 +0000 2015   \n",
       "4           4  637696327485366272  Sat Aug 29 18:40:26 +0000 2015   \n",
       "5           5  637692793083817985  Sat Aug 29 18:26:24 +0000 2015   \n",
       "6           6  637691649943072772  Sat Aug 29 18:21:51 +0000 2015   \n",
       "7           7  637689418472652800  Sat Aug 29 18:12:59 +0000 2015   \n",
       "8           8  637687177946734592  Sat Aug 29 18:04:05 +0000 2015   \n",
       "9           9  637684866906255360  Sat Aug 29 17:54:54 +0000 2015   \n",
       "\n",
       "                                           post_text     user_id  followers  \\\n",
       "0  It's just over 2 years since I was diagnosed w...  1013187241         84   \n",
       "1  It's Sunday, I need a break, so I'm planning t...  1013187241         84   \n",
       "2  Awake but tired. I need to sleep but my brain ...  1013187241         84   \n",
       "3  RT @SewHQ: #Retro bears make perfect gifts and...  1013187241         84   \n",
       "4  It’s hard to say whether packing lists are mak...  1013187241         84   \n",
       "5  Making packing lists is my new hobby... #movin...  1013187241         84   \n",
       "6  At what point does keeping stuff for nostalgic...  1013187241         84   \n",
       "7  Currently in the finding-boxes-of-random-shit ...  1013187241         84   \n",
       "8  Can't be bothered to cook, take away on the wa...  1013187241         84   \n",
       "9  RT @itventsnews: ITV releases promo video for ...  1013187241         84   \n",
       "\n",
       "   friends  favourites  statuses  retweets  label  \n",
       "0      211         251       837         0      1  \n",
       "1      211         251       837         1      1  \n",
       "2      211         251       837         0      1  \n",
       "3      211         251       837         2      1  \n",
       "4      211         251       837         1      1  \n",
       "5      211         251       837         1      1  \n",
       "6      211         251       837         1      1  \n",
       "7      211         251       837         0      1  \n",
       "8      211         251       837         0      1  \n",
       "9      211         251       837        41      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9f18d9",
   "metadata": {},
   "source": [
    "Para propositos de este proyecto, unicamente interesa el texto del post y el label para realizar clasificacion, por lo cual el dataset se reduce unicamente a estas columnas. Sin embargo, para un futuro analisis se podria realizar un modelo que, dado un handle de Twitter, realice webscrapping y analice los tweets del usuario para determinar si tiene depresion o no. Para este proyecto, unicamente se realizara clasificacion en base a un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7682545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s hard to say whether packing lists are mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Making packing lists is my new hobby... #movin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>At what point does keeping stuff for nostalgic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Currently in the finding-boxes-of-random-shit ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can't be bothered to cook, take away on the wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @itventsnews: ITV releases promo video for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           post_text  label\n",
       "0  It's just over 2 years since I was diagnosed w...      1\n",
       "1  It's Sunday, I need a break, so I'm planning t...      1\n",
       "2  Awake but tired. I need to sleep but my brain ...      1\n",
       "3  RT @SewHQ: #Retro bears make perfect gifts and...      1\n",
       "4  It’s hard to say whether packing lists are mak...      1\n",
       "5  Making packing lists is my new hobby... #movin...      1\n",
       "6  At what point does keeping stuff for nostalgic...      1\n",
       "7  Currently in the finding-boxes-of-random-shit ...      1\n",
       "8  Can't be bothered to cook, take away on the wa...      1\n",
       "9  RT @itventsnews: ITV releases promo video for ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw[[\"post_text\", \"label\"]]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb76bc9",
   "metadata": {},
   "source": [
    "# Normalizacion\n",
    "(Normalmente la eliminacion de stop words, junto con lemmatizacion o stemmizacion son parte de la normalizacion del texto, pero planteo la hipotesis de que para un modelo recurrente, la informacion de tener un stop word o de tener un verbo conjugado puede ser mas valioso que solo los stems o lemmas, por lo que no se eliminan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1428cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanco\\AppData\\Local\\Temp\\ipykernel_16248\\1178538166.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"norm_text\"] = data[\"post_text\"].str.replace(r'[^a-zA-Z0-9\\s{1}áéíóúüñÁÉÍÓÚÑ]', '')\n",
      "C:\\Users\\yanco\\AppData\\Local\\Temp\\ipykernel_16248\\1178538166.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"norm_text\"] = data[\"post_text\"].str.replace(r'[^a-zA-Z0-9\\s{1}áéíóúüñÁÉÍÓÚÑ]', '')\n",
      "C:\\Users\\yanco\\AppData\\Local\\Temp\\ipykernel_16248\\1178538166.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"norm_text\"] = data[\"norm_text\"].str.lower().str.strip().str.rstrip('\\n').str.rstrip('\\r\\n')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_text</th>\n",
       "      <th>label</th>\n",
       "      <th>norm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's just over 2 years since I was diagnosed w...</td>\n",
       "      <td>1</td>\n",
       "      <td>its just over 2 years since i was diagnosed wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's Sunday, I need a break, so I'm planning t...</td>\n",
       "      <td>1</td>\n",
       "      <td>its sunday i need a break so im planning to sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awake but tired. I need to sleep but my brain ...</td>\n",
       "      <td>1</td>\n",
       "      <td>awake but tired i need to sleep but my brain h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SewHQ: #Retro bears make perfect gifts and...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt sewhq retro bears make perfect gifts and ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s hard to say whether packing lists are mak...</td>\n",
       "      <td>1</td>\n",
       "      <td>its hard to say whether packing lists are maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Making packing lists is my new hobby... #movin...</td>\n",
       "      <td>1</td>\n",
       "      <td>making packing lists is my new hobby movinghouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>At what point does keeping stuff for nostalgic...</td>\n",
       "      <td>1</td>\n",
       "      <td>at what point does keeping stuff for nostalgic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Currently in the finding-boxes-of-random-shit ...</td>\n",
       "      <td>1</td>\n",
       "      <td>currently in the findingboxesofrandomshit pack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can't be bothered to cook, take away on the wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>cant be bothered to cook take away on the way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @itventsnews: ITV releases promo video for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt itventsnews itv releases promo video for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           post_text  label  \\\n",
       "0  It's just over 2 years since I was diagnosed w...      1   \n",
       "1  It's Sunday, I need a break, so I'm planning t...      1   \n",
       "2  Awake but tired. I need to sleep but my brain ...      1   \n",
       "3  RT @SewHQ: #Retro bears make perfect gifts and...      1   \n",
       "4  It’s hard to say whether packing lists are mak...      1   \n",
       "5  Making packing lists is my new hobby... #movin...      1   \n",
       "6  At what point does keeping stuff for nostalgic...      1   \n",
       "7  Currently in the finding-boxes-of-random-shit ...      1   \n",
       "8  Can't be bothered to cook, take away on the wa...      1   \n",
       "9  RT @itventsnews: ITV releases promo video for ...      1   \n",
       "\n",
       "                                           norm_text  \n",
       "0  its just over 2 years since i was diagnosed wi...  \n",
       "1  its sunday i need a break so im planning to sp...  \n",
       "2  awake but tired i need to sleep but my brain h...  \n",
       "3  rt sewhq retro bears make perfect gifts and ar...  \n",
       "4  its hard to say whether packing lists are maki...  \n",
       "5   making packing lists is my new hobby movinghouse  \n",
       "6  at what point does keeping stuff for nostalgic...  \n",
       "7  currently in the findingboxesofrandomshit pack...  \n",
       "8  cant be bothered to cook take away on the way ...  \n",
       "9  rt itventsnews itv releases promo video for th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"norm_text\"] = data[\"post_text\"].str.replace(r'[^a-zA-Z0-9\\s{1}áéíóúüñÁÉÍÓÚÑ]', '')\n",
    "data[\"norm_text\"] = data[\"norm_text\"].str.lower().str.strip().str.rstrip('\\n').str.rstrip('\\r\\n')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5065c3",
   "metadata": {},
   "source": [
    "### Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14c0f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16872</th>\n",
       "      <td>mhardzsali active na active bah\\r\\n\\r\\npaytfor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>kimberlym06 those are just your shoes lol</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19401</th>\n",
       "      <td>california college san diego national city htt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17231</th>\n",
       "      <td>tweet and retweet \\r\\n\\r\\ngopayt dreamteamyong</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>rt cbs11doug whats it like to be the homeowner...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>lydiamcrtins well she was yes but she wasnt ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12857</th>\n",
       "      <td>salon hey joy this morning was a complete fail...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13404</th>\n",
       "      <td>marclotter realdonaldtrump mikepence    how ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19423</th>\n",
       "      <td>skinovate skin care clinic httpstcoxpfp5yaij5 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <td>gasp you hate me another gasp and my fucks ran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               norm_text  label\n",
       "16872  mhardzsali active na active bah\\r\\n\\r\\npaytfor...      0\n",
       "3684           kimberlym06 those are just your shoes lol      1\n",
       "19401  california college san diego national city htt...      0\n",
       "17231     tweet and retweet \\r\\n\\r\\ngopayt dreamteamyong      0\n",
       "2511   rt cbs11doug whats it like to be the homeowner...      1\n",
       "10322  lydiamcrtins well she was yes but she wasnt ju...      0\n",
       "12857  salon hey joy this morning was a complete fail...      0\n",
       "13404  marclotter realdonaldtrump mikepence    how ma...      0\n",
       "19423  skinovate skin care clinic httpstcoxpfp5yaij5 ...      0\n",
       "5101   gasp you hate me another gasp and my fucks ran...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(data[[\"norm_text\", \"label\"]], test_size=0.2)\n",
    "print(train.shape)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "381a4d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29805"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train[\"norm_text\"].to_numpy())\n",
    "train_sequences = tokenizer.texts_to_sequences(train[\"norm_text\"].to_numpy())\n",
    "test_sequences = tokenizer.texts_to_sequences(test[\"norm_text\"].to_numpy())\n",
    "\n",
    "words = len(tokenizer.word_index)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "580dc976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq = 0\n",
    "for seq in train_sequences:\n",
    "    if len(seq) > max_seq:\n",
    "        max_seq = len(seq)\n",
    "for seq in test_sequences:\n",
    "    if len(seq) > max_seq:\n",
    "        max_seq = len(seq)\n",
    "        \n",
    "max_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fcfcb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 34)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_seq)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_seq)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38151246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train[\"label\"].to_numpy()\n",
    "y_test = test[\"label\"].to_numpy()\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20736f",
   "metadata": {},
   "source": [
    "# Construccion de modelo simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b9906f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 34)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 34, 128)           3815168   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 34, 128)          98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,012,929\n",
      "Trainable params: 4,012,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 128)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "simple_lstm = keras.Model(inputs, outputs)\n",
    "\n",
    "simple_lstm.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "simple_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc27bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 44s 92ms/step - loss: 0.4666 - accuracy: 0.7593 - precision: 0.7569 - val_loss: 0.3248 - val_accuracy: 0.8591 - val_precision: 0.8476\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 35s 88ms/step - loss: 0.1256 - accuracy: 0.9533 - precision: 0.9485 - val_loss: 0.3481 - val_accuracy: 0.8481 - val_precision: 0.8540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c35f00cb80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lstm.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "471497dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 3s 23ms/step - loss: 0.3494 - accuracy: 0.8522 - precision: 0.8515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34942957758903503, 0.8522499799728394, 0.8514562845230103]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lstm.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec594262",
   "metadata": {},
   "source": [
    "Con un modelo relativamente simple hemos logrado nuestro objetivo de obtener un accuracy de 0.85, con un precision bastante similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbebed3",
   "metadata": {},
   "source": [
    "# Experimentando con mas modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e88d22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 64s 120ms/step - loss: 0.4700 - accuracy: 0.7530 - precision: 0.7587 - val_loss: 0.3342 - val_accuracy: 0.8456 - val_precision: 0.8868\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 42s 105ms/step - loss: 0.1285 - accuracy: 0.9505 - precision: 0.9454 - val_loss: 0.3445 - val_accuracy: 0.8675 - val_precision: 0.8751\n",
      "Evalaucion)\n",
      "125/125 [==============================] - 3s 27ms/step - loss: 0.3594 - accuracy: 0.8515 - precision: 0.8512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3594318926334381, 0.8514999747276306, 0.8512396812438965]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 128)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4839d1e",
   "metadata": {},
   "source": [
    "Agregar una capa densa mejoro el accuracy y precision de la validacion, pero empeoro en la prueba, parece indicar un overfitting, tratemos de agregar droput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a49aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 43s 91ms/step - loss: 0.4799 - accuracy: 0.7502 - precision: 0.7546 - val_loss: 0.3308 - val_accuracy: 0.8469 - val_precision: 0.8536\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 39s 98ms/step - loss: 0.1408 - accuracy: 0.9471 - precision: 0.9414 - val_loss: 0.3769 - val_accuracy: 0.8559 - val_precision: 0.8659\n",
      "\n",
      "Evalaucion\n",
      "125/125 [==============================] - 4s 28ms/step - loss: 0.3679 - accuracy: 0.8550 - precision: 0.8560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3679468631744385, 0.8550000190734863, 0.8560273051261902]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 128)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e92f4c",
   "metadata": {},
   "source": [
    "Se logra mejorar el accuracy de prueba marginalmente, que pasa si se agregan mas neuronas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e7385c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 71s 161ms/step - loss: 0.4636 - accuracy: 0.7562 - precision: 0.7611 - val_loss: 0.3189 - val_accuracy: 0.8547 - val_precision: 0.8581\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 57s 144ms/step - loss: 0.1227 - accuracy: 0.9547 - precision: 0.9509 - val_loss: 0.3529 - val_accuracy: 0.8512 - val_precision: 0.8180\n",
      "\n",
      "Evalaucion\n",
      "125/125 [==============================] - 4s 31ms/step - loss: 0.3578 - accuracy: 0.8580 - precision: 0.8219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3578365445137024, 0.8579999804496765, 0.8218818306922913]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 256)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6e3168",
   "metadata": {},
   "source": [
    "Vemos un incremento en el accuracy, pero una decremento en el precision, como este modelo va a servir para ayudar la realizacion de diagnosticos, no es particularmente buena idea permitir esta perdida.\n",
    "\n",
    "Se realiza otra prueba reduciendo las neuronas de la capa densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8812a56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 59s 132ms/step - loss: 0.4656 - accuracy: 0.7620 - precision: 0.7465 - val_loss: 0.3516 - val_accuracy: 0.8369 - val_precision: 0.8856\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 52s 129ms/step - loss: 0.1199 - accuracy: 0.9561 - precision: 0.9504 - val_loss: 0.3334 - val_accuracy: 0.8528 - val_precision: 0.8902\n",
      "\n",
      "Evalaucion\n",
      "125/125 [==============================] - 4s 30ms/step - loss: 0.3358 - accuracy: 0.8515 - precision: 0.8805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33578750491142273, 0.8514999747276306, 0.8804634213447571]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 256)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d35e4",
   "metadata": {},
   "source": [
    "Esta prueba reduce nuestro accuray marginalmente, pero aumenta nuestro precision considerablemente.\n",
    "\n",
    "Que pasa si agregamos una nueva capa recurrente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6be12ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 77s 171ms/step - loss: 0.5220 - accuracy: 0.7312 - precision: 0.7250 - val_loss: 0.3444 - val_accuracy: 0.8441 - val_precision: 0.8288\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 65s 162ms/step - loss: 0.1674 - accuracy: 0.9391 - precision: 0.9318 - val_loss: 0.3856 - val_accuracy: 0.8609 - val_precision: 0.8959\n",
      "\n",
      "Evalaucion\n",
      "125/125 [==============================] - 5s 41ms/step - loss: 0.4035 - accuracy: 0.8512 - precision: 0.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4035157859325409, 0.8512499928474426, 0.8852721452713013]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 256)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467abd2",
   "metadata": {},
   "source": [
    "Se ve un incremento marginal en el precision y un decremento marginal en el accuracy. Aunque se debe notar que hasta ahora se ha tratado de  reducir el nivel de neuronas en cada capa, este es el mejor modelo hasta ahora, veamos que pasa si se cambia la politica de reduccion de neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72f6edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "400/400 [==============================] - 81s 177ms/step - loss: 0.4639 - accuracy: 0.7579 - precision: 0.7466 - val_loss: 0.3186 - val_accuracy: 0.8566 - val_precision: 0.8419\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - 64s 159ms/step - loss: 0.1251 - accuracy: 0.9529 - precision: 0.9452 - val_loss: 0.3470 - val_accuracy: 0.8606 - val_precision: 0.8589\n",
      "\n",
      "Evalaucion\n",
      "125/125 [==============================] - 5s 40ms/step - loss: 0.3523 - accuracy: 0.8615 - precision: 0.8524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3523355722427368, 0.8615000247955322, 0.8523967862129211]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 256)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "x = keras.layers.Dense(128, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a99981",
   "metadata": {},
   "source": [
    "Se logra un incremento en el accuracy pero un decremento considerable en el accuracy, no se tomara en cuenta esta configuracion\n",
    "\n",
    "Se decide entonces entrenar el modelo a lo largo de varias epochs usando la mejor configuracion hasta ahora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70df336",
   "metadata": {},
   "source": [
    "Se juntan los datasets de test y entrenamiento para entrenar todo el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70ee793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 34), (20000,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_x = np.vstack((x_train, x_test))\n",
    "total_y = np.hstack((y_train, y_test))\n",
    "\n",
    "total_x.shape, total_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54bcc5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "625/625 [==============================] - 122s 179ms/step - loss: 0.4222 - accuracy: 0.7865 - precision: 0.7769\n",
      "Epoch 2/25\n",
      "625/625 [==============================] - 119s 190ms/step - loss: 0.1357 - accuracy: 0.9484 - precision: 0.9387\n",
      "Epoch 3/25\n",
      "625/625 [==============================] - 118s 188ms/step - loss: 0.0540 - accuracy: 0.9805 - precision: 0.9776\n",
      "Epoch 4/25\n",
      "625/625 [==============================] - 103s 165ms/step - loss: 0.0323 - accuracy: 0.9893 - precision: 0.9880\n",
      "Epoch 5/25\n",
      "625/625 [==============================] - 104s 166ms/step - loss: 0.0183 - accuracy: 0.9938 - precision: 0.9939\n",
      "Epoch 6/25\n",
      "625/625 [==============================] - 107s 171ms/step - loss: 0.0152 - accuracy: 0.9948 - precision: 0.9942\n",
      "Epoch 7/25\n",
      "625/625 [==============================] - 108s 174ms/step - loss: 0.0111 - accuracy: 0.9964 - precision: 0.9954\n",
      "Epoch 8/25\n",
      "625/625 [==============================] - 119s 191ms/step - loss: 0.0114 - accuracy: 0.9966 - precision: 0.9966\n",
      "Epoch 9/25\n",
      "625/625 [==============================] - 110s 175ms/step - loss: 0.0091 - accuracy: 0.9969 - precision: 0.9968\n",
      "Epoch 10/25\n",
      "625/625 [==============================] - 120s 192ms/step - loss: 0.0093 - accuracy: 0.9974 - precision: 0.9977\n",
      "Epoch 11/25\n",
      "625/625 [==============================] - 95s 151ms/step - loss: 0.0045 - accuracy: 0.9988 - precision: 0.9991\n",
      "Epoch 12/25\n",
      "625/625 [==============================] - 98s 157ms/step - loss: 0.0083 - accuracy: 0.9976 - precision: 0.9977\n",
      "Epoch 13/25\n",
      "625/625 [==============================] - 106s 170ms/step - loss: 0.0040 - accuracy: 0.9987 - precision: 0.9989\n",
      "Epoch 14/25\n",
      "625/625 [==============================] - 104s 167ms/step - loss: 0.0048 - accuracy: 0.9985 - precision: 0.9991\n",
      "Epoch 15/25\n",
      "625/625 [==============================] - 120s 191ms/step - loss: 0.0044 - accuracy: 0.9985 - precision: 0.9984\n",
      "Epoch 16/25\n",
      "625/625 [==============================] - 105s 168ms/step - loss: 0.0036 - accuracy: 0.9990 - precision: 0.9991\n",
      "Epoch 17/25\n",
      "625/625 [==============================] - 104s 166ms/step - loss: 0.0025 - accuracy: 0.9993 - precision: 0.9996\n",
      "Epoch 18/25\n",
      "625/625 [==============================] - 109s 174ms/step - loss: 0.0016 - accuracy: 0.9997 - precision: 0.9997\n",
      "Epoch 19/25\n",
      "625/625 [==============================] - 110s 177ms/step - loss: 0.0026 - accuracy: 0.9991 - precision: 0.9992\n",
      "Epoch 20/25\n",
      "625/625 [==============================] - 117s 188ms/step - loss: 0.0052 - accuracy: 0.9988 - precision: 0.9987\n",
      "Epoch 21/25\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.0018 - accuracy: 0.9996 - precision: 0.9998\n",
      "Epoch 22/25\n",
      "625/625 [==============================] - 140s 223ms/step - loss: 0.0010 - accuracy: 0.9998 - precision: 0.9999\n",
      "Epoch 23/25\n",
      "625/625 [==============================] - 122s 195ms/step - loss: 6.7468e-04 - accuracy: 0.9998 - precision: 1.0000\n",
      "Epoch 24/25\n",
      "625/625 [==============================] - 100s 160ms/step - loss: 0.0022 - accuracy: 0.9993 - precision: 0.9996\n",
      "Epoch 25/25\n",
      "625/625 [==============================] - 100s 159ms/step - loss: 0.0034 - accuracy: 0.9991 - precision: 0.9992\n",
      "\n",
      "Evalaucion\n",
      "125/125 [==============================] - 10s 44ms/step - loss: 0.0117 - accuracy: 0.9962 - precision: 0.9966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.011706149205565453, 0.9962499737739563, 0.9965652823448181]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "inputs = keras.Input(shape=(max_seq,), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(words+1, 256)(inputs)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", keras.metrics.Precision()])\n",
    "\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=10, verbose=0, mode=\"max\")\n",
    "mcp_save = keras.callbacks.ModelCheckpoint(\"tweets_resulting_model.h5\", save_best_only=True, monitor=\"accuracy\", mode=\"max\")\n",
    "\n",
    "model.fit(total_x, total_y, batch_size=32, epochs=25,\n",
    "          callbacks=[earlyStopping, mcp_save])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Evalaucion\")\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
