{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f72b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import optimizers\n",
    "from keras.backend import clear_session\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.metrics import Precision\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180653d",
   "metadata": {},
   "source": [
    "Datos obtenidos de:\n",
    "https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?select=diabetes_012_health_indicators_BRFSS2015.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57551e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253680, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv(\"diabetes_012_health_indicators_BRFSS2015.csv\")\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f0b994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "5           0.0     1.0       1.0        1.0  25.0     1.0     0.0   \n",
       "6           0.0     1.0       0.0        1.0  30.0     1.0     0.0   \n",
       "7           0.0     1.0       1.0        1.0  25.0     1.0     0.0   \n",
       "8           2.0     1.0       1.0        1.0  30.0     1.0     0.0   \n",
       "9           0.0     0.0       0.0        1.0  24.0     0.0     0.0   \n",
       "\n",
       "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                   0.0           0.0     0.0  ...            1.0   \n",
       "1                   0.0           1.0     0.0  ...            0.0   \n",
       "2                   0.0           0.0     1.0  ...            1.0   \n",
       "3                   0.0           1.0     1.0  ...            1.0   \n",
       "4                   0.0           1.0     1.0  ...            1.0   \n",
       "5                   0.0           1.0     1.0  ...            1.0   \n",
       "6                   0.0           0.0     0.0  ...            1.0   \n",
       "7                   0.0           1.0     0.0  ...            1.0   \n",
       "8                   1.0           0.0     1.0  ...            1.0   \n",
       "9                   0.0           0.0     0.0  ...            1.0   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
       "0          0.0      5.0      18.0      15.0       1.0  0.0   9.0        4.0   \n",
       "1          1.0      3.0       0.0       0.0       0.0  0.0   7.0        6.0   \n",
       "2          1.0      5.0      30.0      30.0       1.0  0.0   9.0        4.0   \n",
       "3          0.0      2.0       0.0       0.0       0.0  0.0  11.0        3.0   \n",
       "4          0.0      2.0       3.0       0.0       0.0  0.0  11.0        5.0   \n",
       "5          0.0      2.0       0.0       2.0       0.0  1.0  10.0        6.0   \n",
       "6          0.0      3.0       0.0      14.0       0.0  0.0   9.0        6.0   \n",
       "7          0.0      3.0       0.0       0.0       1.0  0.0  11.0        4.0   \n",
       "8          0.0      5.0      30.0      30.0       1.0  0.0   9.0        5.0   \n",
       "9          0.0      2.0       0.0       0.0       0.0  1.0   8.0        4.0   \n",
       "\n",
       "   Income  \n",
       "0     3.0  \n",
       "1     1.0  \n",
       "2     8.0  \n",
       "3     6.0  \n",
       "4     4.0  \n",
       "5     8.0  \n",
       "6     7.0  \n",
       "7     4.0  \n",
       "8     1.0  \n",
       "9     3.0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04ee8b",
   "metadata": {},
   "source": [
    "En los datos crudos tenemos 21 variables independientes para determinar la variable dependiente objetivo **Diabetes_012**, la cual es una variable categorica con tres posibles resultados, 0, 1 o 2, que significan *no diabetes*, *prediabetes* y *diabetes* respectivamente.\n",
    "\n",
    "Nuestras variables son:\n",
    "1. HighBP, categorica binaria\n",
    "1. HighChol, categorica binaria\n",
    "1. CholCheck, categorica binaria\n",
    "1. BMI, existen 84 valores enteros unicos dentro del dataset y esta sesgado, favoreciendo valores entre 20 y 40\n",
    "1. Smoker, categorica binaria\n",
    "1. Stroke, categorica binaria\n",
    "1. HeartDiseaseorAttack, categorica binaria\n",
    "1. HeartDiseaseorAttack, categorica binaria\n",
    "1. Fruits, categorica binaria\n",
    "1. Veggies, categorica binaria\n",
    "1. HvyAlcoholConsump, categorica binaria\n",
    "1. AnyHealthcare, categorica binaria\n",
    "1. NoDocbcCost, categorica binaria\n",
    "1. GenHlth, categorica con cinco posibles valores\n",
    "1. MentHlth, valores enteros entre 0 y 30 posibles valores con la mayoria de valores siendo 0\n",
    "1. PhysHlth, comportamiento similar a MentHlth\n",
    "1. DiffWalk, categorica binaria\n",
    "1. Sex, categorica binaria\n",
    "1. Age, categoriza rangos de edades en 13 posibles categorias\n",
    "1. Education, categoriza el nivel de educacion en 6 posibles categorias\n",
    "1. Income, categoriza el nivel de ingreso en 8 posibles categorias\n",
    "\n",
    "La gran mayoria de las variables son categoricas binarias, con excepciones para BMI, MentHlth, PhysHlth, Age, Education e Income.\n",
    "\n",
    "Las variables categoricas binarias no necesitan ningun preprocesamiento, pero MentHlth, PhysHlth, Age, Education e Income pasaran por one_hot encoding. Por otra parte, BMI seran tratadas como variable numerica, esta variable presenta comportamiento leptocurtico con asimetria positiva, se le aplicara una transformacion logaritmica para normalizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6187910a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20606617ca0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(raw.BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebf49553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20603996d00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(np.log(raw.BMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d41e286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20672e1e490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(raw.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb2f734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x2067396afd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(raw.Education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56114eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20603865460>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(raw.Income)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0ec15",
   "metadata": {},
   "source": [
    "# Creando nuevo dataset con transformaciones y train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d8041b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>MentHlth_0.0</th>\n",
       "      <th>MentHlth_1.0</th>\n",
       "      <th>MentHlth_2.0</th>\n",
       "      <th>MentHlth_3.0</th>\n",
       "      <th>MentHlth_4.0</th>\n",
       "      <th>MentHlth_5.0</th>\n",
       "      <th>MentHlth_6.0</th>\n",
       "      <th>MentHlth_7.0</th>\n",
       "      <th>MentHlth_8.0</th>\n",
       "      <th>MentHlth_9.0</th>\n",
       "      <th>MentHlth_10.0</th>\n",
       "      <th>MentHlth_11.0</th>\n",
       "      <th>MentHlth_12.0</th>\n",
       "      <th>MentHlth_13.0</th>\n",
       "      <th>MentHlth_14.0</th>\n",
       "      <th>MentHlth_15.0</th>\n",
       "      <th>MentHlth_16.0</th>\n",
       "      <th>MentHlth_17.0</th>\n",
       "      <th>MentHlth_18.0</th>\n",
       "      <th>MentHlth_19.0</th>\n",
       "      <th>MentHlth_20.0</th>\n",
       "      <th>MentHlth_21.0</th>\n",
       "      <th>MentHlth_22.0</th>\n",
       "      <th>...</th>\n",
       "      <th>PhysHlth_18.0</th>\n",
       "      <th>PhysHlth_19.0</th>\n",
       "      <th>PhysHlth_20.0</th>\n",
       "      <th>PhysHlth_21.0</th>\n",
       "      <th>PhysHlth_22.0</th>\n",
       "      <th>PhysHlth_23.0</th>\n",
       "      <th>PhysHlth_24.0</th>\n",
       "      <th>PhysHlth_25.0</th>\n",
       "      <th>PhysHlth_26.0</th>\n",
       "      <th>PhysHlth_27.0</th>\n",
       "      <th>PhysHlth_28.0</th>\n",
       "      <th>PhysHlth_29.0</th>\n",
       "      <th>PhysHlth_30.0</th>\n",
       "      <th>Age_1.0</th>\n",
       "      <th>Age_2.0</th>\n",
       "      <th>Age_3.0</th>\n",
       "      <th>Age_4.0</th>\n",
       "      <th>Age_5.0</th>\n",
       "      <th>Age_6.0</th>\n",
       "      <th>Age_7.0</th>\n",
       "      <th>Age_8.0</th>\n",
       "      <th>Age_9.0</th>\n",
       "      <th>Age_10.0</th>\n",
       "      <th>Age_11.0</th>\n",
       "      <th>Age_12.0</th>\n",
       "      <th>Age_13.0</th>\n",
       "      <th>Education_1.0</th>\n",
       "      <th>Education_2.0</th>\n",
       "      <th>Education_3.0</th>\n",
       "      <th>Education_4.0</th>\n",
       "      <th>Education_5.0</th>\n",
       "      <th>Education_6.0</th>\n",
       "      <th>Income_1.0</th>\n",
       "      <th>Income_2.0</th>\n",
       "      <th>Income_3.0</th>\n",
       "      <th>Income_4.0</th>\n",
       "      <th>Income_5.0</th>\n",
       "      <th>Income_6.0</th>\n",
       "      <th>Income_7.0</th>\n",
       "      <th>Income_8.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  ...  Income_6.0  Income_7.0  Income_8.0\n",
       "0           0.0     1.0       1.0  ...           0           0           0\n",
       "1           0.0     0.0       0.0  ...           0           0           0\n",
       "2           0.0     1.0       1.0  ...           0           0           1\n",
       "3           0.0     1.0       0.0  ...           1           0           0\n",
       "4           0.0     1.0       1.0  ...           0           0           0\n",
       "5           0.0     1.0       1.0  ...           0           0           1\n",
       "6           0.0     1.0       0.0  ...           0           1           0\n",
       "7           0.0     1.0       1.0  ...           0           0           0\n",
       "8           2.0     1.0       1.0  ...           0           0           0\n",
       "9           0.0     0.0       0.0  ...           0           0           0\n",
       "\n",
       "[10 rows x 106 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data = raw, columns = [\"MentHlth\", \"PhysHlth\", \"Age\", \"Education\", \"Income\"])\n",
    "data.BMI = np.log(raw.BMI)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6019e471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202944, 106)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)\n",
    "#train, val = train_test_split(pre_train, test_size=0.1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2476b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202944, 105)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.get_dummies(data = train[[\"Diabetes_012\"]], columns = [\"Diabetes_012\"]).to_numpy()\n",
    "#val_y = pd.get_dummies(data = val[[\"Diabetes_012\"]], columns = [\"Diabetes_012\"]).to_numpy()\n",
    "test_y = pd.get_dummies(data = test[[\"Diabetes_012\"]], columns = [\"Diabetes_012\"]).to_numpy()\n",
    "\n",
    "\n",
    "train_x = train.iloc[:,1:].to_numpy()\n",
    "#val_x = val.iloc[:,1:].to_numpy()\n",
    "test_x = test.iloc[:,1:].to_numpy()\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb70f1b",
   "metadata": {},
   "source": [
    "# Modelado\n",
    "\n",
    "### Creando una ANN simple\n",
    "Este se usara como base a comparar contra futuros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6400a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.4132 - accuracy: 0.8434 - precision: 0.8486 - val_loss: 0.4032 - val_accuracy: 0.8469 - val_precision: 0.85120s - loss: 0.4132 - accuracy: 0.8435 - precision: 0.\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4007 - accuracy: 0.8481 - precision: 0.8530 - val_loss: 0.4018 - val_accuracy: 0.8471 - val_precision: 0.8516\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3970 - accuracy: 0.8486 - precision: 0.8540 - val_loss: 0.3970 - val_accuracy: 0.8479 - val_precision: 0.8520\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3942 - accuracy: 0.8500 - precision: 0.8553 - val_loss: 0.3985 - val_accuracy: 0.8481 - val_precision: 0.8514\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3926 - accuracy: 0.8499 - precision: 0.8553 - val_loss: 0.3984 - val_accuracy: 0.8486 - val_precision: 0.8526\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3905 - accuracy: 0.8503 - precision: 0.8561 - val_loss: 0.4037 - val_accuracy: 0.8469 - val_precision: 0.8495\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3886 - accuracy: 0.8511 - precision: 0.8570 - val_loss: 0.3991 - val_accuracy: 0.8474 - val_precision: 0.8541\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3869 - accuracy: 0.8521 - precision: 0.8576 - val_loss: 0.4004 - val_accuracy: 0.8484 - val_precision: 0.8546\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3854 - accuracy: 0.8528 - precision: 0.8584 - val_loss: 0.4017 - val_accuracy: 0.8460 - val_precision: 0.8512\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3835 - accuracy: 0.8528 - precision: 0.8584 - val_loss: 0.4089 - val_accuracy: 0.8468 - val_precision: 0.8506\n",
      "1586/1586 [==============================] - 1s 723us/step - loss: 0.4139 - accuracy: 0.8456 - precision: 0.8489\n",
      "[0.41387245059013367, 0.8455928564071655, 0.8488783836364746]\n"
     ]
    }
   ],
   "source": [
    "simple_ann = Sequential()\n",
    "simple_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "simple_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "simple_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"adam\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "simple_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(simple_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd24c7a",
   "metadata": {},
   "source": [
    "# Entrenando diferentes modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369aa405",
   "metadata": {},
   "source": [
    "Probando diferentes optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33775d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4118 - accuracy: 0.8437 - precision_1: 0.8493 - val_loss: 0.4042 - val_accuracy: 0.8462 - val_precision_1: 0.8517\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4000 - accuracy: 0.8476 - precision_1: 0.8527 - val_loss: 0.4006 - val_accuracy: 0.8482 - val_precision_1: 0.8525\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3966 - accuracy: 0.8490 - precision_1: 0.8546 - val_loss: 0.3989 - val_accuracy: 0.8484 - val_precision_1: 0.8549\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3945 - accuracy: 0.8501 - precision_1: 0.8557 - val_loss: 0.3988 - val_accuracy: 0.8465 - val_precision_1: 0.8529\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3926 - accuracy: 0.8503 - precision_1: 0.8559 - val_loss: 0.3988 - val_accuracy: 0.8470 - val_precision_1: 0.8561\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3912 - accuracy: 0.8507 - precision_1: 0.8564 - val_loss: 0.3997 - val_accuracy: 0.8479 - val_precision_1: 0.8519\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3889 - accuracy: 0.8514 - precision_1: 0.8571 - val_loss: 0.3997 - val_accuracy: 0.8472 - val_precision_1: 0.8522\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3869 - accuracy: 0.8520 - precision_1: 0.8573 - val_loss: 0.4020 - val_accuracy: 0.8468 - val_precision_1: 0.8529\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3853 - accuracy: 0.8529 - precision_1: 0.8582 - val_loss: 0.4047 - val_accuracy: 0.8480 - val_precision_1: 0.8502\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3835 - accuracy: 0.8530 - precision_1: 0.8585 - val_loss: 0.4026 - val_accuracy: 0.8481 - val_precision_1: 0.8538\n",
      "1586/1586 [==============================] - 1s 846us/step - loss: 0.4058 - accuracy: 0.8463 - precision_1: 0.8525\n",
      "[0.40575331449508667, 0.8462827205657959, 0.8524810671806335]\n"
     ]
    }
   ],
   "source": [
    "simple_ann = Sequential()\n",
    "simple_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "simple_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "simple_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"nadam\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "simple_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(simple_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5204be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4586 - accuracy: 0.8407 - precision_1: 0.8442 - val_loss: 0.4296 - val_accuracy: 0.8416 - val_precision_1: 0.8444\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4268 - accuracy: 0.8427 - precision_1: 0.8459 - val_loss: 0.4223 - val_accuracy: 0.8424 - val_precision_1: 0.8458\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4218 - accuracy: 0.8427 - precision_1: 0.8466 - val_loss: 0.4193 - val_accuracy: 0.8423 - val_precision_1: 0.8469\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 5s 991us/step - loss: 0.4194 - accuracy: 0.8430 - precision_1: 0.8472 - val_loss: 0.4178 - val_accuracy: 0.8426 - val_precision_1: 0.8474\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4180 - accuracy: 0.8430 - precision_1: 0.8474 - val_loss: 0.4166 - val_accuracy: 0.8434 - val_precision_1: 0.8479\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4169 - accuracy: 0.8433 - precision_1: 0.8479 - val_loss: 0.4159 - val_accuracy: 0.8432 - val_precision_1: 0.8479\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4161 - accuracy: 0.8437 - precision_1: 0.8481 - val_loss: 0.4151 - val_accuracy: 0.8436 - val_precision_1: 0.8482\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4155 - accuracy: 0.8439 - precision_1: 0.8483 - val_loss: 0.4146 - val_accuracy: 0.8438 - val_precision_1: 0.8484\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4149 - accuracy: 0.8439 - precision_1: 0.8485 - val_loss: 0.4141 - val_accuracy: 0.8440 - val_precision_1: 0.8487\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.4144 - accuracy: 0.8441 - precision_1: 0.8490 - val_loss: 0.4137 - val_accuracy: 0.8441 - val_precision_1: 0.8486\n",
      "1586/1586 [==============================] - 1s 848us/step - loss: 0.4186 - accuracy: 0.8402 - precision_1: 0.8456\n",
      "[0.41861066222190857, 0.8401923775672913, 0.8456234931945801]\n"
     ]
    }
   ],
   "source": [
    "simple_ann = Sequential()\n",
    "simple_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "simple_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "simple_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"adagrad\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "simple_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(simple_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff336a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4260 - accuracy: 0.8423 - precision: 0.8460 - val_loss: 0.4146 - val_accuracy: 0.8446 - val_precision: 0.8474\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4115 - accuracy: 0.8448 - precision: 0.8496 - val_loss: 0.4079 - val_accuracy: 0.8462 - val_precision: 0.8520\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 5s 981us/step - loss: 0.4073 - accuracy: 0.8459 - precision: 0.8510 - val_loss: 0.4045 - val_accuracy: 0.8460 - val_precision: 0.8513\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.4044 - accuracy: 0.8469 - precision: 0.8516 - val_loss: 0.4024 - val_accuracy: 0.8467 - val_precision: 0.8503\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 5s 998us/step - loss: 0.4018 - accuracy: 0.8478 - precision: 0.8525 - val_loss: 0.4005 - val_accuracy: 0.8475 - val_precision: 0.8540 loss: 0 - ETA: 1s - los\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 5s 968us/step - loss: 0.3998 - accuracy: 0.8488 - precision: 0.8536 - val_loss: 0.3990 - val_accuracy: 0.8480 - val_precision: 0.8527\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.3986 - accuracy: 0.8487 - precision: 0.8536 - val_loss: 0.3971 - val_accuracy: 0.8487 - val_precision: 0.8539\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 5s 979us/step - loss: 0.3974 - accuracy: 0.8487 - precision: 0.8537 - val_loss: 0.3990 - val_accuracy: 0.8477 - val_precision: 0.8506\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 5s 941us/step - loss: 0.3965 - accuracy: 0.8496 - precision: 0.8547 - val_loss: 0.3999 - val_accuracy: 0.8482 - val_precision: 0.8512\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3956 - accuracy: 0.8496 - precision: 0.8544 - val_loss: 0.3997 - val_accuracy: 0.8479 - val_precision: 0.8504\n",
      "1586/1586 [==============================] - 1s 740us/step - loss: 0.4046 - accuracy: 0.8454 - precision: 0.8479\n",
      "[0.40464818477630615, 0.8454352021217346, 0.8479201197624207]\n"
     ]
    }
   ],
   "source": [
    "simple_ann = Sequential()\n",
    "simple_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "simple_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "simple_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"sgd\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "simple_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(simple_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8337e9",
   "metadata": {},
   "source": [
    "Parece que el optimizador de nadam fue provee la mejor combinacion de accuracy y precision, por lo que se experimentarara con las capas usando este estimador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ba18fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 13s 3ms/step - loss: 0.4101 - accuracy: 0.8449 - precision: 0.8495 - val_loss: 0.4112 - val_accuracy: 0.8413 - val_precision: 0.8478\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 13s 3ms/step - loss: 0.4006 - accuracy: 0.8479 - precision: 0.8527 - val_loss: 0.4013 - val_accuracy: 0.8468 - val_precision: 0.8502\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 14s 3ms/step - loss: 0.3977 - accuracy: 0.8484 - precision: 0.8532 - val_loss: 0.4003 - val_accuracy: 0.8477 - val_precision: 0.8537\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 14s 3ms/step - loss: 0.3954 - accuracy: 0.8488 - precision: 0.8540 - val_loss: 0.3973 - val_accuracy: 0.8479 - val_precision: 0.8517\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 13s 3ms/step - loss: 0.3934 - accuracy: 0.8499 - precision: 0.8554 - val_loss: 0.3993 - val_accuracy: 0.8486 - val_precision: 0.8522\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 14s 3ms/step - loss: 0.3915 - accuracy: 0.8499 - precision: 0.8559 - val_loss: 0.3967 - val_accuracy: 0.8485 - val_precision: 0.8534\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 15s 3ms/step - loss: 0.3893 - accuracy: 0.8509 - precision: 0.8567 - val_loss: 0.4003 - val_accuracy: 0.8468 - val_precision: 0.8519\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 15s 3ms/step - loss: 0.3872 - accuracy: 0.8517 - precision: 0.8571 - val_loss: 0.4063 - val_accuracy: 0.8470 - val_precision: 0.8503\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 21s 4ms/step - loss: 0.3846 - accuracy: 0.8531 - precision: 0.8583 - val_loss: 0.4023 - val_accuracy: 0.8471 - val_precision: 0.8500\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 20s 4ms/step - loss: 0.3822 - accuracy: 0.8543 - precision: 0.8594 - val_loss: 0.4006 - val_accuracy: 0.8470 - val_precision: 0.8516\n",
      "1586/1586 [==============================] - 2s 1ms/step - loss: 0.4060 - accuracy: 0.8454 - precision: 0.8496\n",
      "[0.40596437454223633, 0.8453760743141174, 0.8496431112289429]\n"
     ]
    }
   ],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(units = 512, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 256, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 128, activation = \"tanh\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"nadam\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "231ac2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4112 - accuracy: 0.8444 - precision: 0.8494 - val_loss: 0.4013 - val_accuracy: 0.8475 - val_precision: 0.8508\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.4011 - accuracy: 0.8469 - precision: 0.8522 - val_loss: 0.3979 - val_accuracy: 0.8489 - val_precision: 0.8523\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.3991 - accuracy: 0.8471 - precision: 0.8528 - val_loss: 0.4008 - val_accuracy: 0.8469 - val_precision: 0.8487\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3979 - accuracy: 0.8478 - precision: 0.8534 - val_loss: 0.3959 - val_accuracy: 0.8483 - val_precision: 0.8546\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3967 - accuracy: 0.8487 - precision: 0.8541 - val_loss: 0.4074 - val_accuracy: 0.8465 - val_precision: 0.8482\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3959 - accuracy: 0.8494 - precision: 0.8548 - val_loss: 0.3957 - val_accuracy: 0.8496 - val_precision: 0.8530ci\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3950 - accuracy: 0.8492 - precision: 0.8546 - val_loss: 0.3950 - val_accuracy: 0.8491 - val_precision: 0.8557\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3945 - accuracy: 0.8491 - precision: 0.8549 - val_loss: 0.4060 - val_accuracy: 0.8464 - val_precision: 0.8479\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3942 - accuracy: 0.8497 - precision: 0.8551 - val_loss: 0.3947 - val_accuracy: 0.8490 - val_precision: 0.8555\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3930 - accuracy: 0.8499 - precision: 0.8556 - val_loss: 0.4047 - val_accuracy: 0.8434 - val_precision: 0.8559\n",
      "1586/1586 [==============================] - 1s 768us/step - loss: 0.4091 - accuracy: 0.8395 - precision: 0.85210s - loss: 0.4116 - accuracy: 0.8385 - precision:  - ETA: 0s - loss: 0.4119 - accuracy: 0.\n",
      "[0.40912267565727234, 0.8395025134086609, 0.8520733118057251]\n"
     ]
    }
   ],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(units = 210, activation = \"tanh\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"nadam\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02754dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 13s 3ms/step - loss: 0.4106 - accuracy: 0.8448 - precision: 0.8504 - val_loss: 0.4079 - val_accuracy: 0.8451 - val_precision: 0.8607\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 14s 3ms/step - loss: 0.4010 - accuracy: 0.8476 - precision: 0.8530 - val_loss: 0.4014 - val_accuracy: 0.8472 - val_precision: 0.8497\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 14s 3ms/step - loss: 0.3976 - accuracy: 0.8487 - precision: 0.8535 - val_loss: 0.3983 - val_accuracy: 0.8479 - val_precision: 0.8526\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 15s 3ms/step - loss: 0.3952 - accuracy: 0.8493 - precision: 0.8546 - val_loss: 0.3987 - val_accuracy: 0.8473 - val_precision: 0.8540\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 14s 3ms/step - loss: 0.3929 - accuracy: 0.8502 - precision: 0.8559 - val_loss: 0.4030 - val_accuracy: 0.8459 - val_precision: 0.8475\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 15s 3ms/step - loss: 0.3901 - accuracy: 0.8507 - precision: 0.8559 - val_loss: 0.3988 - val_accuracy: 0.8482 - val_precision: 0.8533\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 15s 3ms/step - loss: 0.3874 - accuracy: 0.8522 - precision: 0.8571 - val_loss: 0.3992 - val_accuracy: 0.8471 - val_precision: 0.8526\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 16s 3ms/step - loss: 0.3843 - accuracy: 0.8531 - precision: 0.8579 - val_loss: 0.4014 - val_accuracy: 0.8484 - val_precision: 0.8549\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 17s 3ms/step - loss: 0.3804 - accuracy: 0.8548 - precision: 0.8591 - val_loss: 0.4119 - val_accuracy: 0.8453 - val_precision: 0.8511\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 15s 3ms/step - loss: 0.3766 - accuracy: 0.8554 - precision: 0.8599 - val_loss: 0.4046 - val_accuracy: 0.8465 - val_precision: 0.8512\n",
      "1586/1586 [==============================] - 2s 1ms/step - loss: 0.4080 - accuracy: 0.8442 - precision: 0.8491\n",
      "[0.408005952835083, 0.8442131876945496, 0.8491033911705017]\n"
     ]
    }
   ],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(units = 512, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 256, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 128, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = optimizers.nadam(learning_rate=0.0001)\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55626586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4272 - accuracy: 0.8426 - precision: 0.8472 - val_loss: 0.4124 - val_accuracy: 0.8448 - val_precision: 0.8470\n",
      "Epoch 2/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4070 - accuracy: 0.8459 - precision: 0.8511 - val_loss: 0.4047 - val_accuracy: 0.8467 - val_precision: 0.8511\n",
      "Epoch 3/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4027 - accuracy: 0.8476 - precision: 0.8524 - val_loss: 0.4022 - val_accuracy: 0.8473 - val_precision: 0.8533\n",
      "Epoch 4/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3997 - accuracy: 0.8483 - precision: 0.8534 - val_loss: 0.3993 - val_accuracy: 0.8482 - val_precision: 0.8533\n",
      "Epoch 5/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3973 - accuracy: 0.8490 - precision: 0.8538 - val_loss: 0.3977 - val_accuracy: 0.8488 - val_precision: 0.8526\n",
      "Epoch 6/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3953 - accuracy: 0.8500 - precision: 0.8550 - val_loss: 0.3967 - val_accuracy: 0.8488 - val_precision: 0.8534\n",
      "Epoch 7/10\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3938 - accuracy: 0.8503 - precision: 0.8552 - val_loss: 0.3951 - val_accuracy: 0.8490 - val_precision: 0.8541\n",
      "Epoch 8/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3926 - accuracy: 0.8506 - precision: 0.8554 - val_loss: 0.3948 - val_accuracy: 0.8493 - val_precision: 0.8556\n",
      "Epoch 9/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3915 - accuracy: 0.8506 - precision: 0.8558 - val_loss: 0.3954 - val_accuracy: 0.8490 - val_precision: 0.8547\n",
      "Epoch 10/10\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3907 - accuracy: 0.8512 - precision: 0.8564 - val_loss: 0.3943 - val_accuracy: 0.8493 - val_precision: 0.8552\n",
      "1586/1586 [==============================] - 1s 699us/step - loss: 0.3980 - accuracy: 0.8472 - precision: 0.8531\n",
      "[0.39800435304641724, 0.8472287654876709, 0.8531132340431213]\n"
     ]
    }
   ],
   "source": [
    "simple_ann = Sequential()\n",
    "simple_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "simple_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "simple_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = optimizers.Nadam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "simple_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 10,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(simple_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27add1",
   "metadata": {},
   "source": [
    "Reducir el learning rate ayudo a mejorar el accuracy con el set de prueba, asi que se entrenara un modelo de mas epocas usando un bajo learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d0576f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.5006 - accuracy: 0.8425 - precision_1: 0.8426 - val_loss: 0.4363 - val_accuracy: 0.8429 - val_precision_1: 0.8441\n",
      "Epoch 2/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4280 - accuracy: 0.8430 - precision_1: 0.8456 - val_loss: 0.4209 - val_accuracy: 0.8428 - val_precision_1: 0.8466\n",
      "Epoch 3/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4189 - accuracy: 0.8432 - precision_1: 0.8473 - val_loss: 0.4162 - val_accuracy: 0.8432 - val_precision_1: 0.8473\n",
      "Epoch 4/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4152 - accuracy: 0.8437 - precision_1: 0.8481 - val_loss: 0.4139 - val_accuracy: 0.8436 - val_precision_1: 0.8485\n",
      "Epoch 5/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4132 - accuracy: 0.8443 - precision_1: 0.8488 - val_loss: 0.4125 - val_accuracy: 0.8443 - val_precision_1: 0.8490\n",
      "Epoch 6/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4119 - accuracy: 0.8445 - precision_1: 0.8493 - val_loss: 0.4115 - val_accuracy: 0.8447 - val_precision_1: 0.8490\n",
      "Epoch 7/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4108 - accuracy: 0.8448 - precision_1: 0.8498 - val_loss: 0.4106 - val_accuracy: 0.8444 - val_precision_1: 0.8503\n",
      "Epoch 8/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4099 - accuracy: 0.8452 - precision_1: 0.8504 - val_loss: 0.4099 - val_accuracy: 0.8452 - val_precision_1: 0.8502\n",
      "Epoch 9/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4091 - accuracy: 0.8453 - precision_1: 0.8505 - val_loss: 0.4093 - val_accuracy: 0.8451 - val_precision_1: 0.8516\n",
      "Epoch 10/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4084 - accuracy: 0.8455 - precision_1: 0.8506 - val_loss: 0.4086 - val_accuracy: 0.8457 - val_precision_1: 0.8513\n",
      "Epoch 11/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4077 - accuracy: 0.8458 - precision_1: 0.8509 - val_loss: 0.4080 - val_accuracy: 0.8461 - val_precision_1: 0.8510\n",
      "Epoch 12/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4071 - accuracy: 0.8460 - precision_1: 0.8509 - val_loss: 0.4075 - val_accuracy: 0.8461 - val_precision_1: 0.8513\n",
      "Epoch 13/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4065 - accuracy: 0.8463 - precision_1: 0.8510 - val_loss: 0.4070 - val_accuracy: 0.8460 - val_precision_1: 0.8510\n",
      "Epoch 14/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4059 - accuracy: 0.8463 - precision_1: 0.8513 - val_loss: 0.4066 - val_accuracy: 0.8463 - val_precision_1: 0.8519\n",
      "Epoch 15/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4054 - accuracy: 0.8464 - precision_1: 0.8514 - val_loss: 0.4060 - val_accuracy: 0.8463 - val_precision_1: 0.8515\n",
      "Epoch 16/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4049 - accuracy: 0.8468 - precision_1: 0.8514 - val_loss: 0.4056 - val_accuracy: 0.8465 - val_precision_1: 0.8517\n",
      "Epoch 17/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4044 - accuracy: 0.8467 - precision_1: 0.8516 - val_loss: 0.4052 - val_accuracy: 0.8466 - val_precision_1: 0.8522\n",
      "Epoch 18/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4039 - accuracy: 0.8470 - precision_1: 0.8518 - val_loss: 0.4048 - val_accuracy: 0.8469 - val_precision_1: 0.8518\n",
      "Epoch 19/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4035 - accuracy: 0.8471 - precision_1: 0.8523 - val_loss: 0.4044 - val_accuracy: 0.8468 - val_precision_1: 0.8518\n",
      "Epoch 20/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4030 - accuracy: 0.8473 - precision_1: 0.8524 - val_loss: 0.4043 - val_accuracy: 0.8465 - val_precision_1: 0.8510\n",
      "Epoch 21/50\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.4026 - accuracy: 0.8473 - precision_1: 0.8525 - val_loss: 0.4037 - val_accuracy: 0.8471 - val_precision_1: 0.8521\n",
      "Epoch 22/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4022 - accuracy: 0.8474 - precision_1: 0.8526 - val_loss: 0.4037 - val_accuracy: 0.8470 - val_precision_1: 0.8529\n",
      "Epoch 23/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4018 - accuracy: 0.8476 - precision_1: 0.8530 - val_loss: 0.4030 - val_accuracy: 0.8472 - val_precision_1: 0.8521\n",
      "Epoch 24/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4014 - accuracy: 0.8477 - precision_1: 0.8530 - val_loss: 0.4027 - val_accuracy: 0.8471 - val_precision_1: 0.8513\n",
      "Epoch 25/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4010 - accuracy: 0.8479 - precision_1: 0.8530 - val_loss: 0.4024 - val_accuracy: 0.8471 - val_precision_1: 0.8516\n",
      "Epoch 26/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4007 - accuracy: 0.8480 - precision_1: 0.8532 - val_loss: 0.4021 - val_accuracy: 0.8471 - val_precision_1: 0.8524\n",
      "Epoch 27/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4003 - accuracy: 0.8481 - precision_1: 0.8533 - val_loss: 0.4020 - val_accuracy: 0.8472 - val_precision_1: 0.8528\n",
      "Epoch 28/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.4000 - accuracy: 0.8483 - precision_1: 0.8535 - val_loss: 0.4015 - val_accuracy: 0.8473 - val_precision_1: 0.8526\n",
      "Epoch 29/50\n",
      "5074/5074 [==============================] - 5s 1ms/step - loss: 0.3996 - accuracy: 0.8486 - precision_1: 0.8538 - val_loss: 0.4012 - val_accuracy: 0.8474 - val_precision_1: 0.8527\n",
      "Epoch 30/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3993 - accuracy: 0.8486 - precision_1: 0.8537 - val_loss: 0.4011 - val_accuracy: 0.8469 - val_precision_1: 0.8526\n",
      "Epoch 31/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3990 - accuracy: 0.8489 - precision_1: 0.8538 - val_loss: 0.4007 - val_accuracy: 0.8474 - val_precision_1: 0.8527\n",
      "Epoch 32/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3987 - accuracy: 0.8489 - precision_1: 0.8538 - val_loss: 0.4005 - val_accuracy: 0.8476 - val_precision_1: 0.8523\n",
      "Epoch 33/50\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3983 - accuracy: 0.8491 - precision_1: 0.8541 - val_loss: 0.4002 - val_accuracy: 0.8477 - val_precision_1: 0.8527\n",
      "Epoch 34/50\n",
      "5074/5074 [==============================] - 8s 2ms/step - loss: 0.3980 - accuracy: 0.8491 - precision_1: 0.8543 - val_loss: 0.4000 - val_accuracy: 0.8473 - val_precision_1: 0.8528\n",
      "Epoch 35/50\n",
      "5074/5074 [==============================] - 8s 2ms/step - loss: 0.3977 - accuracy: 0.8492 - precision_1: 0.8543 - val_loss: 0.3998 - val_accuracy: 0.8473 - val_precision_1: 0.8528\n",
      "Epoch 36/50\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3975 - accuracy: 0.8493 - precision_1: 0.8544 - val_loss: 0.3995 - val_accuracy: 0.8477 - val_precision_1: 0.8524\n",
      "Epoch 37/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3971 - accuracy: 0.8495 - precision_1: 0.8546 - val_loss: 0.3994 - val_accuracy: 0.8479 - val_precision_1: 0.8516\n",
      "Epoch 38/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3969 - accuracy: 0.8496 - precision_1: 0.8544 - val_loss: 0.3993 - val_accuracy: 0.8476 - val_precision_1: 0.8535\n",
      "Epoch 39/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3966 - accuracy: 0.8497 - precision_1: 0.8549 - val_loss: 0.3990 - val_accuracy: 0.8477 - val_precision_1: 0.8531\n",
      "Epoch 40/50\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3964 - accuracy: 0.8500 - precision_1: 0.8548 - val_loss: 0.3987 - val_accuracy: 0.8478 - val_precision_1: 0.8525\n",
      "Epoch 41/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3961 - accuracy: 0.8498 - precision_1: 0.8547 - val_loss: 0.3985 - val_accuracy: 0.8478 - val_precision_1: 0.8529\n",
      "Epoch 42/50\n",
      "5074/5074 [==============================] - 9s 2ms/step - loss: 0.3959 - accuracy: 0.8501 - precision_1: 0.8550 - val_loss: 0.3984 - val_accuracy: 0.8481 - val_precision_1: 0.8535\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3956 - accuracy: 0.8501 - precision_1: 0.8551 - val_loss: 0.3981 - val_accuracy: 0.8480 - val_precision_1: 0.8531\n",
      "Epoch 44/50\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3954 - accuracy: 0.8502 - precision_1: 0.8551 - val_loss: 0.3980 - val_accuracy: 0.8480 - val_precision_1: 0.8535\n",
      "Epoch 45/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3951 - accuracy: 0.8502 - precision_1: 0.8550 - val_loss: 0.3978 - val_accuracy: 0.8479 - val_precision_1: 0.8529\n",
      "Epoch 46/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3949 - accuracy: 0.8503 - precision_1: 0.8555 - val_loss: 0.3977 - val_accuracy: 0.8478 - val_precision_1: 0.8526\n",
      "Epoch 47/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3946 - accuracy: 0.8504 - precision_1: 0.8553 - val_loss: 0.3975 - val_accuracy: 0.8481 - val_precision_1: 0.8531\n",
      "Epoch 48/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3945 - accuracy: 0.8507 - precision_1: 0.8554 - val_loss: 0.3974 - val_accuracy: 0.8482 - val_precision_1: 0.8538\n",
      "Epoch 49/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3942 - accuracy: 0.8506 - precision_1: 0.8555 - val_loss: 0.3974 - val_accuracy: 0.8475 - val_precision_1: 0.8524\n",
      "Epoch 50/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3941 - accuracy: 0.8504 - precision_1: 0.8555 - val_loss: 0.3971 - val_accuracy: 0.8484 - val_precision_1: 0.8537\n",
      "1586/1586 [==============================] - 1s 719us/step - loss: 0.4007 - accuracy: 0.8466 - precision_1: 0.8518\n",
      "[0.4006926119327545, 0.8466177582740784, 0.8518407344818115]\n"
     ]
    }
   ],
   "source": [
    "simple_ann = Sequential()\n",
    "simple_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "simple_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "simple_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = optimizers.Nadam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "simple_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 50,\n",
    "              validation_split = 0.2)\n",
    "\n",
    "print(simple_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e940a6",
   "metadata": {},
   "source": [
    "Se observa un comportamiento oscilatorio en cuanto al accuracy del set de validacion y el de prueba no llega a 0.85, sin embargo en ambos casos el accuracy si se aproxima a 0.85.\n",
    "\n",
    "En este caso, se decide mejor trabajar con un modelo mas sencillo y no sacrificar computo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecfb25ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.4108 - accuracy: 0.8441 - precision_1: 0.8496 - val_loss: 0.4037 - val_accuracy: 0.8469 - val_precision_1: 0.8514\n",
      "Epoch 2/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3996 - accuracy: 0.8479 - precision_1: 0.8529 - val_loss: 0.3999 - val_accuracy: 0.8473 - val_precision_1: 0.8537accuracy: 0.8480 - precision_1: 0.85\n",
      "Epoch 3/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3964 - accuracy: 0.8493 - precision_1: 0.8546 - val_loss: 0.3972 - val_accuracy: 0.8489 - val_precision_1: 0.8549\n",
      "Epoch 4/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3945 - accuracy: 0.8492 - precision_1: 0.8548 - val_loss: 0.3989 - val_accuracy: 0.8468 - val_precision_1: 0.8525\n",
      "Epoch 5/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3927 - accuracy: 0.8501 - precision_1: 0.8556 - val_loss: 0.3984 - val_accuracy: 0.8482 - val_precision_1: 0.8552\n",
      "Epoch 6/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3910 - accuracy: 0.8508 - precision_1: 0.8565 - val_loss: 0.3976 - val_accuracy: 0.8487 - val_precision_1: 0.8529\n",
      "Epoch 7/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3892 - accuracy: 0.8509 - precision_1: 0.8568 - val_loss: 0.3971 - val_accuracy: 0.8490 - val_precision_1: 0.8545\n",
      "Epoch 8/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3878 - accuracy: 0.8519 - precision_1: 0.8571 - val_loss: 0.4014 - val_accuracy: 0.8466 - val_precision_1: 0.8520\n",
      "Epoch 9/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3856 - accuracy: 0.8520 - precision_1: 0.8579 - val_loss: 0.4016 - val_accuracy: 0.8472 - val_precision_1: 0.8532\n",
      "Epoch 10/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3840 - accuracy: 0.8528 - precision_1: 0.8584 - val_loss: 0.4019 - val_accuracy: 0.8476 - val_precision_1: 0.8514\n",
      "Epoch 11/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3822 - accuracy: 0.8536 - precision_1: 0.8590 - val_loss: 0.4034 - val_accuracy: 0.8478 - val_precision_1: 0.8513\n",
      "Epoch 12/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3802 - accuracy: 0.8540 - precision_1: 0.8598 - val_loss: 0.4017 - val_accuracy: 0.8462 - val_precision_1: 0.8530\n",
      "Epoch 13/50\n",
      "5074/5074 [==============================] - 7s 1ms/step - loss: 0.3783 - accuracy: 0.8542 - precision_1: 0.8597 - val_loss: 0.4068 - val_accuracy: 0.8461 - val_precision_1: 0.8516\n",
      "Epoch 14/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3766 - accuracy: 0.8554 - precision_1: 0.8606 - val_loss: 0.4066 - val_accuracy: 0.8475 - val_precision_1: 0.8518\n",
      "Epoch 15/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3750 - accuracy: 0.8556 - precision_1: 0.8610 - val_loss: 0.4084 - val_accuracy: 0.8460 - val_precision_1: 0.8512\n",
      "Epoch 16/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3735 - accuracy: 0.8564 - precision_1: 0.8618 - val_loss: 0.4114 - val_accuracy: 0.8453 - val_precision_1: 0.8480\n",
      "Epoch 17/50\n",
      "5074/5074 [==============================] - 6s 1ms/step - loss: 0.3717 - accuracy: 0.8574 - precision_1: 0.8627 - val_loss: 0.4119 - val_accuracy: 0.8455 - val_precision_1: 0.8521\n",
      "1586/1586 [==============================] - 1s 668us/step - loss: 0.4170 - accuracy: 0.8422 - precision_1: 0.8493\n",
      "[0.4170244038105011, 0.8421633839607239, 0.8493175506591797]\n"
     ]
    }
   ],
   "source": [
    "final_ann = Sequential()\n",
    "final_ann.add(Dense(units = 210, activation = \"relu\", input_shape = (train_x.shape[1], )))\n",
    "final_ann.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "final_ann.compile(loss = \"categorical_crossentropy\",\n",
    "                   optimizer = \"nadam\",\n",
    "                   metrics = [\"accuracy\", Precision()])\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=10, verbose=0, mode=\"max\")\n",
    "mcp_save = ModelCheckpoint(\"diabetes_resulting_model.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "\n",
    "final_ann.fit(train_x, train_y,\n",
    "              batch_size = 32,\n",
    "              epochs = 50,\n",
    "              validation_split = 0.2,\n",
    "             callbacks=[earlyStopping, mcp_save])\n",
    "\n",
    "print(final_ann.evaluate(test_x, test_y))\n",
    "clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
